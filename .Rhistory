all_categorical() ~ "{n}/{N} ({p}%)"
),
digits = all_continuous() ~ 0,
label = list(price = "Precio",
surface_total = "Superficie total",
surface_covered = "Superficie cubierta",
bedrooms = "Número de cuartos",
bathrooms = "Número de baños"),
missing_text = "(Observaciones faltantes)") |>
modify_header(label ~ "**Variable**") |>
modify_footnote_header("Para variables continuas, media (min, max).",
columns = all_stat_cols()) |>
bold_labels() |>
italicize_levels() |>
modify_caption("**Tabla 1.** Estadística descriptiva.")
#detallitos
####superficie negtaiva pasa a valor absoluto
####superficie total debería ser mayor a la sup cubierta
db <- db |> mutate(surface_total = abs(surface_total))
db <- db |>
mutate(num_pm2 = round(price/surface_total, 0)) |>
mutate(num_pm2 = num_pm2/1000000) |>  # Precio por metro cuadrado en millones.
filter(num_pm2 <= 10)
# Calculamos el ancho de banda mediante la regla de Scott.
num_binwidth <- 3.5 * sd(db$num_pm2, na.rm = TRUE) / length(db$num_pm2[!is.na(db$num_pm2)])^(1/3)
num_binwidth <- round(x = num_binwidth, digits = 2)
plot_pm2 <- db |>
ggplot(aes(x = num_pm2)) +
geom_histogram(mapping = aes(y = (after_stat(count))/sum(after_stat(count))),
color ="#FFFFFF", fill= "#3a5e8cFF", show.legend = FALSE, na.rm = TRUE,
binwidth = num_binwidth, linewidth = 0.2, alpha = 0.8, closed = 'left') +
scale_y_continuous(expand = expansion(mult = c(0, 0.05)),
labels = scales::label_number(scale = 100)) +
scale_x_continuous(expand = expansion(mult = c(0, 0)), limits = c(0, NA)) +
labs(x = "Precio por metro cuadrado", y = "Porcentaje de observaciones (%)",
caption = paste0("Nota. Se eliminaron precios mayores a 10 millones por metro cuadrado.",
'Esto corresponde a 37 observaciones en total.')) +
theme_classic()
plot_pm2
######################
####IMPUTACIÓN########
######################
vis_dat(db)
db_imputation <- db |>
select(surface_total, surface_covered, bedrooms, bathrooms) |>
drop_na(surface_total, surface_covered, bedrooms, bathrooms)
set.seed(2025)
index_test <- sample(x = 1:nrow(db_imputation),
size = round(0.2 * nrow(db_imputation)),
replace = FALSE)
db_test <- db_imputation[index_test, ]
db_train <- db_imputation[-index_test, ]
cv_folds <- vfold_cv(data = db_train, v = 5)
recipe_lm <- recipe(formula = surface_total ~ bedrooms + bathrooms,
data = db_train)
base_lm <- linear_reg()
workflow_lm <- workflow() |>
add_recipe(recipe_lm) |>
add_model(base_lm)
tune_lm <- tune_grid(workflow_lm,
resamples = cv_folds,
metrics = metric_set(rmse))
collect_metrics(tune_lm)
#imputación con bagging
recipe_bag <- recipe(formula = surface_total ~ bedrooms + bathrooms,
data = db_train)
base_bag <- rand_forest(mtry = 2, trees = tune(), min_n = tune()) |>
set_engine('ranger', importance = 'permutation') |>
set_mode('regression')
workflow_bag <- workflow() |>
add_recipe(recipe_bag) |>
add_model(base_bag)
grid_values <- grid_regular(trees(), min_n(),
levels = c('trees' = 4, 'min_n' = 4))
tune_bag <- tune_grid(workflow_bag,
resamples = cv_folds,
grid = grid_values,
metrics = metric_set(rmse))
collect_metrics(tune_bag)
final_bag <- finalize_workflow(workflow_bag, select_best(tune_bag, metric = 'rmse'))
fit_bag <- fit(final_bag, data = db_train)
augment(fit_bag, new_data = db_test) |>
rmse(truth = surface_total, estimate = .pred)
leaflet() |>
addTiles()
db <- db |>
filter(!is.na(lat) & !is.na(lon))
leaflet() |>
addTiles() |>
addCircles(lng = db$lon,
lat = db$lat)
limites <- getbb("Cali Colombia")
limites
db <- db |>
filter(
between(lon, limites[1, "min"], limites[1, "max"]) &
between(lat, limites[2, "min"], limites[2, "max"])
)
leaflet() |>
addTiles() |> #añade mapa
addCircles(lng = db$lon, #circulos para cada vivienda
lat = db$lat)
db <- db |>
mutate(color = case_when(property_type == "Apartamento" ~ "#2A9D8F",
property_type == "Casa" ~ "#F4A261"))
# Vamos a crear un mensaje en popup con html
html <- paste0("<b>Precio:</b> ",
scales::dollar(db$price),
"<br> <b>Area:</b> ",
as.integer(db$surface_total), " mt2",
"<br> <b>Tipo de immueble:</b> ",
db$property_type,
"<br> <b>Numero de alcobas:</b> ",
as.integer(db$rooms),
"<br> <b>Numero de baños:</b> ",
as.integer(db$bathrooms),
"<br> <b>Sector:</b> ",
db$l4,
"<br> <b>Barrio:</b> ",
db$l5)
# Calculamos el queremos que sea el centro del mapa.
latitud_central <- mean(db$lat)
longitud_central <- mean(db$lon)
# Creamos el gráfico.
leaflet() |>
addTiles() |>
setView(lng = longitud_central, lat = latitud_central, zoom = 12) |>
addCircles(lng = db$lon,
lat = db$lat,
col = db$color,
fillOpacity = 1,
opacity = 1,
radius = 1,
popup = html)
#Ahora en ggplot
comunas <- st_read("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/comunas.geojson")
comunas <- st_transform(comunas, 4626)
ggplot() +
geom_sf(data = comunas, color = "red")
sf_db <- st_as_sf(db, coords = c("lon", "lat"),  crs = 4626)
ggplot() +
geom_sf(data = comunas, color = "red") +
geom_sf(data = sf_db |>
filter(property_type== "Apartamento"),
aes(color = num_pm2) ,shape = 15, size = 0.3) +
theme_classic()
# Posibles categorias de las que podemos extraer información geosespacial.
osmdata::available_tags("leisure")
parques <- opq(bbox = getbb("Cali Colombia")) |>
add_osm_feature(key = "leisure" , value = "park")
# Cambiamos el formato para que sea un objeto sf (simple features)
parques_sf <- osmdata_sf(parques)
parques_geometria <- parques_sf$osm_polygons |>
dplyr::select(osm_id, name)
parques_geometria <- st_as_sf(parques_sf$osm_polygons)
centroides <- st_centroid(parques_geometria, byid = TRUE)
centroides <- centroides |>
mutate(x=st_coordinates(centroides)[, "X"]) |>
mutate(y=st_coordinates(centroides)[, "Y"])
leaflet() |>
addTiles() |>
setView(lng = longitud_central, lat = latitud_central, zoom = 12) |>
addPolygons(data = parques_geometria, col = "red",weight = 10,
opacity = 0.8, popup = parques_geometria$name) |>
addCircles(lng = centroides$x,
lat = centroides$y,
col = "darkblue", opacity = 0.5, radius = 1)
centroides_sf <- st_as_sf(centroides, coords = c("x", "y"), crs=4326)
sf_db <- st_as_sf(db, coords = c("lon", "lat"),  crs = 4326)
View(centroides)
# ¡ADVERTENCIA! Esto va a ser demorado...
dist_matrix <- st_distance(x = sf_db, y = centroides_sf)
dim(dist_matrix)
dist_min <- apply(dist_matrix, 1, min)
db <- db |> mutate(distancia_parque = dist_min)
# Calculamos el ancho de banda mediante la regla de Scott.
num_binwidth <- 3.5 * sd(db$distancia_parque, na.rm = TRUE) / length(db$distancia_parque[!is.na(db$distancia_parque)])^(1/3)
num_binwidth <- round(x = num_binwidth, digits = 2)
db |>
ggplot(aes(x = distancia_parque)) +
geom_histogram(mapping = aes(y = (after_stat(count))/sum(after_stat(count))),
color ="#FFFFFF", fill= "#3a5e8cFF", show.legend = FALSE, na.rm = TRUE,
binwidth = num_binwidth, linewidth = 0.2, alpha = 0.8, closed = 'left') +
scale_y_continuous(expand = expansion(mult = c(0, 0.05)),
labels = scales::label_number(scale = 100)) +
scale_x_continuous(expand = expansion(mult = c(0, 0)), limits = c(0, NA)) +
labs(x = "Distancia al parque más cercano", y = "Porcentaje de observaciones (%)") +
theme_classic()
library(tensorflow)
tensorflow::install_tensorflow(envname = 'r-reticulate')
reticulate::py_discover_config()
library(reticulate)
Sys.setenv((RETICULATE_PYTHON = "C:/Users/catal/Documents/.virtualenvs/r-reticulate/Scripts/python.exe"))
library(reticulate)
Sys.setenv((RETICULATE_PYTHON = "C:\Users\catal\Documents\.virtualenvs\r-reticulate\Scripts\python.exe"))
library(reticulate)
Sys.setenv((RETICULATE_PYTHON = "C:/Users/catal/Documents/.virtualenvs/r-reticulate/Scripts/python.exe"))
Sys.setenv(RETICULATE_PYTHON = "C:/Users/catal/Documents/.virtualenvs/r-reticulate/Scripts/python.exe"))
Sys.setenv(RETICULATE_PYTHON = "C:/Users/catal/Documents/.virtualenvs/r-reticulate/Scripts/python.exe")
use_python(Sys.getenv("RETICULATE_PYTHON"), required = TRUE)
py_config()
install_keras(envname = "r-reticulate")
library(keras)
install_keras(envname = "r-reticulate")
library(keras)
install_keras(envname = "r-reticulate")
reticulate::py_discover_config()
setwd(dirname(dirname(rstudioapi::getActiveDocumentContext()$path)))
library(pacman)
p_load(
rio, tidyverse, caret, sf, spatialsample, glmnet
)
train <- read_csv("stores/train_final.csv")
test  <- read_csv("stores/test_final.csv")
vars <- c("property_type", "LocNombre", "SCANOMBRE", "CODIGO_UPZ")
train[vars] <- lapply(train[vars], as.factor)
test[vars]  <- lapply(test[vars],  as.factor)
train <- train |>
mutate(across(where(~ is.numeric(.x) && all(.x %in% c(0,1))), ~ factor(.x)))
test <- test |>
mutate(across(where(~ is.numeric(.x) && all(.x %in% c(0,1))), ~ factor(.x)))
train$...1 <- NULL
test$...1  <- NULL
train <- train %>% drop_na()
train <- train |> select(where(~ n_distinct(.x) > 1))
test <- test %>%
mutate(area = ifelse(is.na(area), median(area, na.rm = TRUE), area))
espec_modelo <- log(price) ~ property_type + cocina_americana + cocina_integral +
gimnasio + balcon + chimenea + terraza + ascensor + sauna + jacuzzi +
piscina + deposito + walking_closet + duplex + zona_verde + bbq +
conjunto_residencial + altillo + vigilancia_24h + porteria + cctv +
parqueadero_cubierto + parqueadero_comunal + zona_infantil +
salon_comunal + zona_humeda + terraza_comunal + pet_friendly +
remodelado + piso_madera + piso_porcelanato + n_parqueaderos +
banios + area + habitaciones + LocNombre + EPE + EPT + EPCC +
distnearestlibrary + distnearestschool + distnearestmuseum +
distnearesttransmi + recaudo_predial + lon + lat
ctrl <- trainControl(
method = "cv",
number = 5,
savePredictions = TRUE
)
set.seed(1234)
lasso_model <- train(
espec_modelo,
data = train,
metric = "MAE",
method = "glmnet",
trControl = ctrl,
family = "gaussian",
tuneGrid = expand.grid(
alpha  = 1,                                # LASSO
lambda = 10^seq(-3, 3, length = 50)        # wide lambda grid
)
)
lasso_model
best_lambda <- lasso_model$bestTune$lambda
best_lambda
# Extract coefficients
coef_lasso <- coef(lasso_model$finalModel, s = best_lambda)
View(coef_lasso)
coef_df <- as.matrix(coef_lasso) %>%
as.data.frame() %>%
rownames_to_column("variable") %>%
rename(coef = 2) %>%
filter(variable != "(Intercept)") %>%
mutate(abs_coef = abs(coef)) %>%
arrange(desc(abs_coef)) %>%
filter(!if_any(everything(), ~ grepl("Loc", .x)))
variables_selected <- coef_df$variable[1:15]
variables_selected <- gsub("1","",variables_selected)
variables_selected <- gsub("Casa","",variables_selected)
variables_lasso <- c("property_id","price","LocNombre","habitaciones","distnearestlibrary","distnearestschool",
"distnearestmuseum","distnearesttransmi",variables_selected)
train_lasso <- train %>% select(all_of(variables_lasso))
test_lasso <- test %>% select(all_of(variables_lasso))
write.csv(train_lasso,"stores/train_final_lasso.csv")
write.csv(test_lasso,"stores/test_final_lasso,csv")
write.csv(test_lasso,"stores/test_final_lasso.csv")
View(train_lasso)
rm(list=ls())
setwd(choose.dir())
library(pacman)
p_load(rio,       # Import/export data.
tidyverse, # Tidy-data.
caret,     # For predictive model assessment.
sf,           # Manejo de datos espaciales.
spatialsample, # Validación cruzada espacial
tidymodels,
leaps)     # For subset  model selection
train <- read_csv("stores/train_final_lasso.csv")
test  <- read_csv("stores/test_final_lasso.csv")
View(test)
vars <- c("property_type", "LocNombre")
train[vars] <- lapply(train[vars], as.factor)
test[vars] <- lapply(test[vars], as.factor)
train <- train |>
mutate(across(
where(~ is.numeric(.x) && all(.x %in% c(0,1))),
~ factor(.x)
))
test <- test |>
mutate(across(
where(~ is.numeric(.x) && all(.x %in% c(0,1))),
~ factor(.x)
))
train$...1 <- NULL
test$...1 <- NULL
train <- train %>% tidyr::drop_na()
train <- train |>
select(where(~ n_distinct(.x) > 1))
test <- test %>%
mutate(
area = ifelse(is.na(area), median(area, na.rm = TRUE), area)
)
train <- read_csv("stores/train_final.csv")
test  <- read_csv("stores/test_final.csv")
vars <- c("property_type", "LocNombre")
train[vars] <- lapply(train[vars], as.factor)
test[vars] <- lapply(test[vars], as.factor)
train <- train |>
mutate(across(
where(~ is.numeric(.x) && all(.x %in% c(0,1))),
~ factor(.x)
))
test <- test |>
mutate(across(
where(~ is.numeric(.x) && all(.x %in% c(0,1))),
~ factor(.x)
))
train$...1 <- NULL
test$...1 <- NULL
train <- train %>% tidyr::drop_na()
train <- train |>
select(where(~ n_distinct(.x) > 1))
test <- test %>%
mutate(
area = ifelse(is.na(area), median(area, na.rm = TRUE), area)
)
train_lasso <- read_csv("stores/train_final_lasso.csv")
train <- train %>%
mutate(log_price = log(price))
train_sf <- st_as_sf(train, coords = c("lon", "lat"), crs = 4326) %>%
st_transform(3116)
set.seed(2025)
block_folds <- spatial_block_cv(train_sf, v = 5)
autoplot(block_folds)
espec_modelo_xgb <- as.formula(
paste("log_price ~ property_type + gimnasio +
balcon + chimenea + terraza + ascensor + jacuzzi + piscina + deposito +
walking_closet + zona_verde + cctv + parqueadero_cubierto + parqueadero_comunal +
zona_humeda + n_parqueaderos + banios + area +
habitaciones + distnearestlibrary + distnearestschool + distnearestmuseum +
distnearesttransmi + recaudo_predial")
)
rec_xgb <- recipes::recipe(
espec_modelo_xgb  , data = train) %>%
step_novel(all_nominal_predictors()) %>% # Categoría para las clases no vistas en el train.
step_dummy(all_nominal_predictors()) %>%  # Variables binarias de las categóricas.
step_zv(all_predictors()) %>%   #  Elimina predictores con varianza cero (constantes).
step_normalize(all_predictors())  # Estandariza los predictores.
xgb_spec <- boost_tree(
mode           = "regression",
trees          = tune(),  # nrounds
tree_depth     = tune(),  # max_depth
learn_rate     = tune(),  # eta
loss_reduction = tune(),  # gamma
min_n          = tune(),  # min_child_weight
sample_size    = tune(),  # subsample
mtry           = tune()   # colsample_bytree
) %>%
set_engine("xgboost")
xgb_wf <- workflow() %>%
add_model(xgb_spec) %>%
add_recipe(rec_xgb)
# definimos rangos razonables
trees_range      <- trees(c(200, 1000))          # número de árboles
tree_depth_range <- tree_depth(c(2L, 8L))        # profundidad del árbol
learn_rate_range <- learn_rate(c(-3, 0))         # log10 scale: 10^-3 a 10^0
loss_red_range   <- loss_reduction(c(-3, 1))     # gamma (log10)
min_n_range      <- min_n(c(2L, 20L))            # min_child_weight
sample_range     <- sample_prop(c(0.5, 1.0))     # subsample
mtry_range       <- mtry(c(5L, 40L))             # depende de # de predictores
grid_xgb <- grid_latin_hypercube(
trees_range,
tree_depth_range,
learn_rate_range,
loss_red_range,
min_n_range,
sample_range,
mtry_range,
size = 30  # número de combinaciones
)
set.seed(123)
xgb_res <- tune_grid(
xgb_wf,
resamples = block_folds,
grid      = grid_xgb,
metrics   = metric_set(mae),
control   = control_grid(verbose = TRUE)
)
install.packages("xgboost")
library(xgboost)
set.seed(123)
xgb_res <- tune_grid(
xgb_wf,
resamples = block_folds,
grid      = grid_xgb,
metrics   = metric_set(mae),
control   = control_grid(verbose = TRUE)
)
best_xgb <- select_best(xgb_res, "mae")
best_xgb <- select_best(xgb_res, metric="mae")
best_xgb
xgb_final_wf <- finalize_workflow(xgb_wf, best_xgb)
xgb_final_fit <- fit(xgb_final_wf, data = train_sf)
xgb_pred_test <- augment(xgb_final_fit, new_data = test) %>%
mutate(
price_hat       = exp(.pred),
price_hat_round = round(price_hat / 100000) * 100000
) %>%
select(id, price_hat_round)
xgb_pred_test <- augment(xgb_final_fit, new_data = test) %>%
mutate(
price_hat       = exp(.pred),
price_hat_round = round(price_hat / 100000) * 100000
) %>%
select(propery_id, price_hat_round)
#predicciones
xgb_pred_test <- augment(xgb_final_fit, new_data = test) %>%
mutate(
price_hat       = exp(.pred),
price_hat_round = round(price_hat / 100000) * 100000
) %>%
select(property_id, price_hat_round)
write_csv(final_submit, "XGB_mtry10_tree707_min_n18_depth7_lrate_0027_cvspatial.csv")
write_csv(xgb_pred_test, "XGB_mtry10_tree707_min_n18_depth7_lrate_0027_cvspatial.csv")
best_xgb
xgb_res
espacial_metrics <- collect_metrics(xgb_res)
mae_espacial <- collect_metrics(xgb_res) %>%
filter(.metric == "mae") %>%
select(mean, std_err) %>%
mutate(tipo = "CV espacial")
set.seed(2025)
folds_normal <- vfold_cv(
train_sf,
v = 5,                 # número de folds (como el v de spatial_block_cv)
strata = log_price     # opcional pero recomendado para regresión
)
set.seed(123)
xgb_res_normal <- tune_grid(
xgb_wf,
resamples = folds_normal,
grid      = grid_xgb,
metrics   = metric_set(mae),
control   = control_grid(verbose = TRUE)
)
xgb_res_normal
mae_normal <- collect_metrics(xgb_res_normal) %>%
filter(.metric == "mae") %>%
select(mean, std_err) %>%
mutate(tipo = "CV tradicional")
best_xgb_normal <- select_best(xgb_res_normal, metric="mae")
best_xgb_normal
xgb_final_wf_n <- finalize_workflow(xgb_wf, best_xgb_normal)
xgb_final_fit_n <- fit(xgb_final_wf_n, data = train_sf)
xgb_pred_test_norm <- augment(xgb_final_fit_n, new_data = test) %>%
mutate(
price_hat       = exp(.pred),
price_hat_round = round(price_hat / 100000) * 100000
) %>%
select(property_id, price_hat_round)
write_csv(xgb_pred_test, "XGB_mtry30_tree941_min_n3_depth5_lrate_0262_cvspatial.csv")
write_csv(xgb_pred_test_norm, "XGB_mtry30_tree941_min_n3_depth5_lrate_0262_cvnormal.csv")
comparacion_mae <- bind_rows(mae_espacial, mae_normal)
comparacion_mae
comparacion_mae %>%
ggplot(aes(x = tipo, y = mean, fill = tipo)) +
geom_col() +
labs(
title = "Comparación del MAE",
y = "MAE promedio",
x = ""
) +
theme_minimal()
View(comparacion_mae)
comparacion_mae %>%
ggplot(aes(x = tipo, y = mean, fill = tipo)) +
geom_col() +
labs(
title = "Comparación del MAE",
y = "MAE promedio",
x = ""
) +
theme_minimal() + coord_cartesian(ylim = c(45, 50))
comparacion_mae %>%
group_by(tipo) %>%
summarise(mae_promedio = mean(mean))
maecvspatial_en_pesos <- mean(abs(train_sf$price - xgb_pred_test$price_hat_round))
maecvnormal_en_pesos <- mean(abs(train_sf$price - xgb_pred_test_norm$price_hat_round))
comparacion_mae %>%
group_by(tipo) %>%
comparacion_mae %>%
group_by(tipo) %>%
summarise(mae_promedio = mean(mean))
comparacion_mae <- bind_rows(mae_espacial, mae_normal)
comparacion_mae
comparacion_mae %>%
group_by(tipo) %>%
summarise(mae_promedio = mean(mean))
saveRDS(comparacion_mae, "comp_mae_xgboost.rds")
